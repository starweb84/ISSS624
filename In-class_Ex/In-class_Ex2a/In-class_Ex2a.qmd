---
title: "In-class Exercise 2a: Emerging Hot Spot Analysis: sfdep methods"
date: "25 November 2023"
date-modified: "last-modified"
format: html
execute:
  echo: true
  eval: true
  warning: false
editor: visual
---

## **1. Overview**

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:

-   Building a space-time cube,

-   Calculating Getis-Ord local Gi\* statistic for each bin by using an FDR correction,

-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test,

-   Categorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.

## **Getting started**

### **Installing and Loading the R Packages**

Six R packages will be used for this in-class exercise, they are sf, sfdep, tmap, tidyverse and knitr.

```{r}
#| code-fold: true
#| code-summary: "Show the code" 
pacman::p_load(sf, sfdep, tmap, tidyverse, knitr,plotly)
```

## **The Data**

For the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:

Hunan, a geospatial data set in ESRI shapefile format, and
Hunan_GDPPC, an attribute data set in csv format.
Before getting started, reveal the content of Hunan_GDPPC.csv by using Notepad and MS Excel.

## **Getting the Data Into R Environment**

### **Import Geospatial data into r environment**

The code chunk below uses [*st_read()*](https://r-spatial.github.io/sf/reference/st_read.html) of **sf** package to import Hunan shapefile into R. The imported shapefile will be **simple features** Object of **sf**.

```{r}
#| code-fold: true
#| code-summary: "Show the code" 
hunan <- st_read(dsn = "data/geospatial",layer = "Hunan")
```

### **Import attribute table into r environment**

Next, we will import *Hunan_GDPPC.csv* into R by using *read_csv()* of **readr** package. The output is R dataframe class.

```{r}
#| code-fold: true
#| code-summary: "Show the code" 
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

### **Creating a time series cube**

```{r}

GDPPC_st <- spacetime (GDPPC, hunan,
                       .loc_col = "County",
                       .time_col = "Year")
```

Check if time series cube created.

```{r}
is_spacetime_cube(GDPPC_st)
```

### **Computing Gi\***

Next, we will compute the local Gi\* statistics.

### **Deriving the spatial weights**

```{r}

GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate (nb = include_self(st_contiguity(geometry)),
          wt =st_inverse_distance (nb, geometry,
                                   scale = 1,
                                   alpha = 1),
          .before = 1) %>%
  set_nbs ("nb") %>%
  set_wts ("wt")

```

### **Things to learn from the code chunk above**

-   `activate()` of dplyr package is used to activate the geometry context

-   `mutate()` of dplyr package is used to create two new columns *nb* and *wt*.

-   Then we will activate the data context again and copy over the nb and wt columns to each time-slice using `set_nbs()` and `set_wts()`

    -   row order is very important so do not rearrange the observations after using `set_nbs()` or `set_wts()`.

    Note that this dataset now has neighbors and weights for each time-slice.

```{r}
head(GDPPC_nb)
```

**Computing Gi\***

We can use these new columns to manually calculate the local Gi\* for each location. We can do this by grouping by *Year* and using `local_gstar_perm()` of sfdep package. After which, we `use unnest()` to unnest *gi_star* column of the newly created *gi_starts* data.frame.

```{r}
gi_stars <- GDPPC_nb %>%
  group_by(Year) %>%
  mutate (gi_star = local_gstar_perm(GDPPC,nb,wt))%>%
  tidyr::unnest(gi_star)
```

# **Mann-Kendall Test**

With these Gi\* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county.

```{r}
cbg <- gi_stars %>% 
  ungroup() %>% 
  filter(County == "Changsha") |> 
  select(County, Year, gi_star)
```

Next, we plot the result by using ggplot2 functions.

```{r}
ggplot(data = cbg, 
       aes(x = Year, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```

We can also create an interactive plot by using ggplotly() of plotly package.

```{r}
p <- ggplot(data = cbg, 
       aes(x = Year, 
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

In the above result, sl is the p-value. This result tells us that there is a slight upward but insignificant trend.

We can replicate this for each location by using group_by() of dplyr package.

```{r}
ehsa <- gi_stars %>%
  group_by(County) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

## **Arrange to show significant emerging hot/cold spots**

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)
```

### \*Performing Emerging hotspot analysis\*\*

Lastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.

```{r}

ehsa <- emerging_hotspot_analysis(x = GDPPC_st,
                                  .var = "GDPPC",
                                  k = 1,
                                  nsim = 99)
```

p-value of ESHA is the permuated value.

### **Visualising the distribution of EHSA classes**

In the code chunk below, ggplot2 functions is used to reveal the distribution of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

Figure above shows that sporadic cold spots class has the high numbers of county.

### **Visualising EHSA**

In this section, you will learn how to visualise the geographic distribution EHSA classes. However, before we can do so, we need to join both *hunan* and *ehsa* together by using the code chunk below.

```{r}
hunan_ehsa <- hunan %>%
  left_join(ehsa,
            by = join_by(County == location))
```

Next, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.

```{r}
ehsa_sig <- hunan_ehsa  %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(hunan_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```

